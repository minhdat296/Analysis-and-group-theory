\section{Hilbert spaces} \label{section: hilbert_spaces}
    \subsection{Hilbert spaces}
        \begin{convention}
            The complex conjugation of some complex number $z := a + ib$ shall be denoted by $z^{\dagger} := a - ib$.
        \end{convention}
    
        \begin{definition}[Hilbert spaces] \label{def: hilbert_spaces}
            Let $\sigma$ be a field automorphism of $K$.
        
            A \textbf{Hilbert space} is a vector space space $H$ equipped with a $\sigma$-braided, positive-definite, and non-degenerate $K$-bilinear form $\<-, -\>_H: H \x H \to K$ (called an \textbf{inner product}), with the braidedness condition meaning that:
                $$\forall x, y \in H: \<x, y\>_H = \sigma( \<y, x\>_H )$$
            The inner product $\<-, -\>$ is said to be \textbf{symmetric} if and only if $\sigma = \id_K$, and \textbf{Hermitian} if and only if $\sigma = (-)^{\dagger}$; note that for complete archimedean local fields, these are the only two possible field automorphisms (since $\Gal(\bbC/\R) \cong \Z/2$). 
        \end{definition}
        \begin{proposition}[Induced norms] \label{prop: induced_norms_on_hilbert_spaces}
            Let $H$ be a Hilbert space. Then, there will be an induced norm $\norm{-}_H$ on $H$ given by:
                $$\forall x \in H: \norm{x}_H := \sqrt{\<x, x\>_H}$$
            When equipped with the topology induced by this norm, Hilbert spaces become normed spaces. 
        \end{proposition}
            \begin{proof}
                Firstly, let us note that $\norm{-}_H$ is a well-defined function $H \to \R_{\geq 0}$ due to the positive-semi-definiteness of $\<-, -\>_H$. Next, the triangle inequality is due to the Cauchy-Schwarz inequality. Finally, to show that $\norm{x}_H = 0$ if and only if $x = 0$, we can simply make use of positive-definiteness.
            \end{proof}
        \begin{corollary}[The Parallelogram Law] \label{coro: parallelogram_law}
            Let $H$ be Hilbert space. Then, for all $x, y \in H$, we have the following identities:
            \begin{enumerate}
                \item $\norm{x - y}_H^2 + \norm{x + y}_H^2 = 2( \norm{x}_H^2 + \norm{y}_H^2 )$.
                \item $\Re \<x, y\>_H = \frac12( \norm{x}_H^2 + \norm{y}_H^2 )$.
                \item $\Im \<x, y\>_H = \frac12( \norm{x}_H^2 + \norm{iy}_H^2 )$.
                \item $\<x, y\>_H = \frac14 \sum_{\e \in \{\pm 1, \pm i\}} \e \norm{x + \e y}_H^2$.
            \end{enumerate}
        \end{corollary}
            \begin{proof}
                \begin{enumerate}
                    \item Consider the following:
                        $$
                            \begin{aligned}
                                \norm{x - y}_H^2 + \norm{x + y}_H^2 & = \<x - y, x - y\>_H + \< x + y, x + y \>_H
                                \\
                                & = \left( \<x, x\>_H - \<y, x\>_H - \<x, y\>_H + \<y, y\>_H \right) + \left( \<x, x\>_H + \<y, x\>_H + \<x, y\>_H + \<y, y\>_H \right)
                                \\
                                & = 2\left( \<x, x\>_H + \<y, y\>_H \right)
                                \\
                                & = 2\left( \norm{x}_H^2 + \norm{y}^2_H \right)
                            \end{aligned}
                        $$
                    \item Consider the following:
                        $$
                            \begin{aligned}
                                \<x, y\>_H & = \norm{x}_H^2 + \norm{y}^2_H - \<y, x\>_H
                                \\
                                & = \norm{x}_H^2 + \norm{y}^2_H - \<x, y\>_H^{\dagger}
                            \end{aligned}
                        $$
                    From this, we infer that:
                        $$2\Re( \<x, y\>_H ) = \<x, y\>_H + \<x, y\>_H^{\dagger} = \norm{x}_H^2 + \norm{y}^2_H$$
                    and the claim follows.
                    \item Similar to the previous part.
                    \item We have:
                        $$
                            \begin{aligned}
                                \<x, y\>_H & = \Re\<x, y\>_H + i \Im \<x, y\>_H
                                \\
                                & = \frac12\left( \norm{x}_H^2 + \norm{y}^2_H \right) + \frac12 i \left( \norm{x}_H^2 + \norm{iy}^2_H \right)
                            \end{aligned}
                        $$
                    and then we can simply apply the first part to each of the terms.
                \end{enumerate}
            \end{proof}
            
        \begin{convention}
            \textit{A priori}, Hilbert spaces are not complete (at least according to definition \ref{def: hilbert_spaces}; it is common in the literature to require that Hilbert spaces are complete right from the beginning, but we find this to be circular\footnote{From the beginning, Hilbert spaces do not carry any topology, so it does not make any sense to require them to be complete. Only after we have shown that inner products induce norms on Hilbert spaces can we meaningfully require them to be complete.}). However, let us assume from now on that every Hilbert space is complete with respect to its norm topology. In particular, this means that Hilbert spaces are now Banach spaces whose norms come from inner products.
        \end{convention}
        The following is arguably the most ubiquitous and most useful family of examples of Hilbert spaces.  
        \begin{example}[$L^2$-spaces] \label{example: L_2_spaces_as_hilbert_spaces}
            Let $(X, \mu)$ be a measure space. In theorem \ref{theorem: L_p_space_duality}, it is shown that for any $p, q \in \N_{\geq 1} \cup \{+\infty\}$ such that $\frac1p + \frac1q = 1$, one has a linear homeomorphism:
                $$L^q(X, \mu) \xrightarrow[]{\cong} L^p(X, \mu)^*_{\weak}$$
            that is given by:
                $$g \mapsto \int_X (-) g d\mu$$

            Now, observe that when $p = q = 2$, not only does this weak duality holds, but it is in fact a weak \textit{self}-duality of $L^2(X, \mu)$, which allows us to define a bilinear pairing on this Banach space by:
                $$\<f, g\>_{L^2(X, \mu)} := \int_X \abs{ fg } d\mu$$
            using which the linear homeomorphism from before can be now alternatively given by $g \mapsto \<-, g\>$. One then verifies that:
                $$\norm{f}_{L^2(X, \mu)}^2 = \int_X \abs{f}^2 d\mu = \<f, f\>_{L^2(X, \mu)}$$
            to see that the induced norm coincides with the $L^2$-norm.
        \end{example}
        \begin{example}[Finite-dimensional Hilbert spaces]
            Any finite-dimensional inner product space is a Hilbert space. In fact, any finite-dimensional vector space can be upgraded to a Hilbert space in a canonical manner: by letting the inner product be the dot product (but of course, there are inner products that are not the dot product).
        \end{example}

        Our next goal shall be to justify the attitude that many \say{reasonable} Hilbert spaces should be regarded as $L^2$-spaces over some particular measure spaces, with the point being that once a Hilbert space $H$ is realisable as an $L^2$-space, say $H \cong L^2(X, \mu)$ isometrically, the inner product on $H$ can be computed as the $L^1$-pairing on $(X, \mu)$, i.e.:
            $$\<f, g\>_H = \int_X \abs{fg} d\mu$$
        In many cases, the integral is usually easier to compute. In a more casual manner, such realisations allows us to regard general Hilbert spaces as direct generalisations of Euclidean spaces, and often, one can use Euclidean intuition for the purposes of studying Hilbert spaces. We caution the reader, though, that not every Hilbert space is an $L^2$-spaces. So-called \say{Sobolev spaces} (see subsection \ref{subsection: sobolev_spaces}), for instances, are such examples. 
        \begin{convention}
            From now on, we will mostly be working inside a fixed Hilbert space and not be concerned too much with morphisms between different Hilbert spaces. Therefore, in order to avoid visual clutter, let us suppress the subscripts indicating where norms and inner products are being taken until the end of the section, unless we fear it may be confusing to do so.
        \end{convention}
        We begin with the following fundamental definition.
        \begin{definition}[Orthogonal subsets of Hilbert spaces] \label{def: orthogonal_subsets}
            A subset $A$ of a Hilbert space $H$ is said to be \textbf{orthogonal} if and only if for all $x \not = y \in A$, we have $\<x, y\> = 0$.
        \end{definition}
        \begin{lemma}[Linear independence of orthogonal subsets] \label{lemma: orthogonal_subsets_linear_independence}
            Any orthogonal subset of a Hilbert space is necessarily linearly independent. 
        \end{lemma}
            \begin{proof}
                Let $H$ be a Hilbert space and let $A \subset H$ be an orthogonal subset. Suppose for the sake of deriving a contradiction that $A$ is linearly dependent, i.e. there exists $x \in A$ such that $x = \sum_i a_i e_i$ for some collection $\{e_i\}_{i \in I} \subset A$ and $\{a_i\}_{i \in I} \subset K \setminus \{0\}$. Then, for any $j \not = i \in I$, we shall have:
                    $$\<x, e_j\> = \sum_{i \in I} a_i \<e_i, e_j\> = \sum_{i \in I} a_i \delta_{i, j} = a_j \not = 0$$
                If $\<x, e_j\> \not = 0$ then $x \in K e_j$, but this would imply that $a_i = 0$ for all $i \not = j$, a contradiction.
            \end{proof}
            
        Orthogonal sequences are rather special, because they form the basis\footnote{Pun intended} for what is known as \say{harmonic analysis} (or \say{Fourier theory}). At the most holistic level, this is essentially just the idea that one can represent elements of a Hilbert space in terms of its projections onto the coordinate axes, though in the infinite-dimensional setting, there are certain technical subtleties that need to be handled with care.

        \begin{proposition}[Pythagorean Theorem] \label{prop: pythagorean_theorem}
            Let $H$ be a Hilbert space and let $\{x_k\}_{k = 1}^n \subset H$ be a finite orthogonal sequence. Then:
                $$\sum_{k = 1}^n \norm{x_k}^2 = \norm{ \sum_{k = 1}^n x_k }^2$$
        \end{proposition}
            \begin{proof}
                By linear independence of orthogonal sequences (lemma \ref{lemma: orthogonal_subsets_linear_independence}), we have an isomorphism of normed spaces:
                    $$\span \{x_k\}_{k = 1}^n \cong K x_1 \oplus_2 ... \oplus_2 K x_n$$
                From this, we infer that:
                    $$\sum_{k = 1}^n \norm{x_k}^2 = \norm{ (x_1, ..., x_n) }_{ K x_1 \oplus_2 ... \oplus_2 K x_n } = \norm{ \sum_{k = 1}^n x_k }^2$$
            \end{proof}
        
        \begin{lemma}[A convergence criterion for orthogonal series] \label{lemma: convergent_orthogonal_series_in_hilbert_spaces}
            Let $H$ be a Hilbert space and let $\{x_k\}_{k \geq 1} \subset H$ be an orthogonal sequence. Then $\sum_{k = 1}^{+\infty} x_k$ is convergent if and only if $\sum_{k = 1}^{+\infty} \norm{x_k}^2$ converges, and when $\sum_{k = 1}^{+\infty} x_k$ converges, we have that $\norm{ \sum_{k = 1}^{+\infty} x_k }^2 = \sum_{k = 1}^{+\infty} \norm{x_k}^2$.
        \end{lemma}
            \begin{proof}
                If $\sum_{k = 1}^{+\infty} \norm{x_k}^2$ is convergent then the sequence of partial sums $\left\{ \sum_{k = 1}^n \norm{x_k}^2 \right\}_{n \geq 1} \subset \R$ will converge, and since $\R$ is complete, this sequence of partial sums converges if and only if it is Cauchy, i.e. for all $\e > 0$ there exists $N \in \N$ such that if $n \geq m \geq N$ then $\abs{ \sum_{k = m}^n \norm{x_k}^2 } = \sum_{k = m}^n \norm{x_k}^2 = \norm{ \sum_{k = m}^n x_k }^2 < \e^2$, with the latter equality holding thanks to proposition \ref{prop: pythagorean_theorem}. The function $(-)^{\frac12}$ is monotonic on $\R_{\geq 0}$, so by applying it to both sides of the inequality above, we shall get $\norm{\sum_{k = m}^n x_k} < \e$ whenever $n \geq m \geq N$, and hence the sequence $\left\{ \sum_{k = 1}^n x_k \right\}_{n \geq 1}$ is Cauchy. This is a Cauchy sequence in a Hilbert space, which is complete by definition, and hence it is convergent, i.e. $\sum_{k = 1}^{+\infty} x_k$ converges in $H$.

                Conversely, if $\sum_{k = 1}^{+\infty} x_k$ is convergent, then because Hilbert spaces are complete, the sequence of partial sums $\left\{ \sum_{k = 1}^n x_k \right\}_{n \geq 1}$ shall be Cauchy, i.e. for all $\e > 0$ there exists $N \in \N$ such that $\norm{ \sum_{k = m}^n x_k } < \e^{\frac12}$ for all $n \geq m \geq N$. Proposition \ref{prop: pythagorean_theorem} tells us that $\sum_{k = m}^n \norm{x_k}^2 = \norm{\sum_{k = m}^n x_k}^2$, so for the same $N \in \N$, we have that $\sum_{k = m}^n \norm{x_k}^2 < (\e^{\frac12})^2 = \e$ for all $n \geq m \geq N$. The sequence of partial sums $\left\{ \sum_{k = 1}^n \norm{x_k}^2 \right\}_{n \geq 1} \subset \R$ is thus Cauchy, and hence convergent, as $\R$ is complete.

                Finally, using again the fact $\sum_{k = 1}^n \norm{x_k}^2 = \norm{\sum_{k = 1}^n x_k}^2$ for all $n \geq 1$ (proposition \ref{prop: pythagorean_theorem}) in conjunction with continuity of norms, we see that when $\sum_{k = 1}^{+\infty} x_k := \lim_{n \to +\infty} \sum_{k = 1}^n x_k$ is convergent, we shall have:
                    $$\norm{\lim_{n \to +\infty} \sum_{k = 1}^n x_k} = \lim_{n \to +\infty} \norm{\sum_{k = 1}^n x_k} = \lim_{n \to +\infty} \sum_{k = 1}^n \norm{x_k}^2 = \sum_{k = 1}^{+\infty} \norm{x_k}^2$$
                as claimed.
            \end{proof}

        The following result shows that orthogonal projection in general Hilbert spaces works as one would expect the process to work in finite-dimensional Euclidean spaces.
        \begin{lemma}[Convex projection] \label{lemma: convex_projection}
            Let $X$ be a normed space. Suppose also tha the norm $\norm{-}_X$ is convex - i.e. for all $t \in [0, 1]$ and all $x, y \in X$, one has that $\norm{ t x + (1 - t) y }_X \leq t \norm{x}_X + (1 - t) \norm{y}_X$. Then, for any non-empty, closed, and convex subset $A \subseteq X$, there exists a unique element $\bar{x} \in A$ such that:
                $$\dist(x, A) = \norm{x - \bar{x}}_X$$
            for all $x \in H$.
        \end{lemma}
            \begin{proof}
                \todo[inline]{Convex projection}
            \end{proof}
        \begin{proposition}[Orthogonal projections onto closed subspaces in Hilbert spaces] \label{prop: orthogonal_projections_onto_closed_subspaces_in_hilbert_spaces}
            Let $H$ be a Hilbert space and let $L \subset H$ be a closed subspace. Then, the orthogonal complement/annihilator $L^{\perp}$ as in corollary \ref{coro: annihilators_vs_orthogonal_complements} is identified as:
                $$L^{\perp} \cong \{ x \in H \mid \forall y \in L: \<x, y\> = 0 \}$$
            and of course, we have $H \cong L \oplus L^{\perp}$ as normed spaces.

            Moreover, for all elements $x \in H$, we have that:
                $$\dist(x, L) := \inf_{y \in L} \norm{x - y} = \norm{x - \pi_L(x)} = \norm{\pi_{L^{\perp}}(x)}$$
            where $\pi_L: H \to L, \pi_{L^{\perp}}: H \to L^{\perp}$ are the canonical projections.
        \end{proposition}
            \begin{proof}
                The key observation here is that Hilbert space norms are convex, and that closed vector subspaces thereof are convex, so lemma \ref{lemma: convex_projection} applies.
            \end{proof}
        \begin{definition}[Idempotents and projections] \label{def: projections}
            We say that an element $P \in E$ of an associative algebra $E$ is an \textbf{idempotent} if and only if $P^2 = P$.
            
            If $X$ is a normed space, then a \textbf{projection} from $X$ shall be an idempotent $P \in \End_{K, \cont}(X)$ such that $\ker P \cong (\im P)^{\perp}$.
        \end{definition}
        \begin{lemma}[Some properties of idempotent continuous operators] \label{lemma: properties_of_idempotent_continuous_operators}
            (Cf. \cite[Proposition II.3.2]{conway_functional_analysis}) Let $X$ be a normed space and $P \in \End_{K, \cont}(X)$ be a continuous operator on $X$.
            \begin{enumerate}
                \item $P$ is an idempotent if and only if $\id_X - P$ is idempotent.
                \item We have:
                    $$\im P \cong \ker(\id_X - P), \ker P \cong \im(\id_X - P)$$
                and both $\ker P$ and $\im P \subset X$ are closed. 
                \item $X \cong \ker P \oplus \im P$.
            \end{enumerate}
        \end{lemma}
            \begin{proof}
                \begin{enumerate}
                    \item Firstly, suppose that $P$ is an idempotent and then consider $(\id_X - P)^2 = \id_X - 2P + P^2 = \id_X - 2P + P = \id_X - P$, which shows that $\id_X - P \in \End_{K, \cont}(X)$ is an idempotent. Conversely, suppose that $\id_X - P \in \End_{K, \cont}(X)$ is an idempotent. Then $\id_X - P = (\id_X - P)^2 = \id_X - 2P + P^2$, from which we gather that $0 = -P + P^2$, i.e. $P^2 = P$.
                    \item 
                    \item 
                \end{enumerate}
            \end{proof}
        \begin{proposition}[Which continuous linear maps are orthogonal projections ?] \label{prop: which_continuous_linear_maps_are_orthogonal_projections}
            (Cf. \cite[Proposition II.3.3]{conway_functional_analysis}) Let $H$ be a Hilbert space and $P \in \End_{K, \cont}(H)$ be idempotent. The following statements are equivalent:
            \begin{enumerate}
                \item $P$ is a projection.
                \item $P = \pi_{\im P}$, i.e. $P$ is the orthogonal projection of $H$ onto $\im P$.
                \item $\norm{E} = 1$.
            \end{enumerate}
        \end{proposition}
            \begin{proof}
                \begin{enumerate}
                    \item 
                    \item 
                    \item 
                \end{enumerate}
            \end{proof}
        \begin{convention}
            Typically, if $H$ is a Hilbert space and $L$ is a closed subspace therein, and if $\pi_L: H \to L, \pi_{L^{\perp}}: H \to L^{\perp}$ are as in proposition \ref{prop: orthogonal_projections_onto_closed_subspaces_in_hilbert_spaces}, then for all $x \in H$, we shall write:
                $$x_L := \pi_L(x), x_{L^{\perp}} := \pi_{L^{\perp}}(x)$$
        \end{convention}
        Finite-dimensional subspaces are closed, so we have the following corollary to proposition \ref{prop: orthogonal_projections_onto_closed_subspaces_in_hilbert_spaces}.
        \begin{corollary}[Projecting onto finite-dimensional subspaces] \label{coro: orthogonal_projections_onto_finite_dimensional_subspaces_in_hilbert_spaces}
            Let $H$ be a Hilbert space and let $E \subset $ be a finite-dimensional subspace, in which we choose an orthogonal sequence $\{e_n\}_{n = 1}^{\dim E} \not \ni 0$. Then:
                $$x_E = \sum_{n = 1}^{\dim E} \frac{\<x, e_n\>}{\norm{e_n}^2} e_n$$
            for all $x \in H$.

            Moreover, we have:
                $$\norm{x_E}^2 = \sum_{n = 1}^{\dim E} \frac{ \abs{ \<x, e_n\> }^2 }{\norm{e_n}^2}$$
        \end{corollary}
            \begin{proof}
                Let $\pi: H \to E$ be the linear map given by $\pi(x) := \sum_{n = 1}^{\dim E} \frac{\<x, e_n\>}{\norm{e_n}^2} e_n$. If we can show that $x - \pi(x) \in E^{\perp}$ then by proposition \ref{prop: orthogonal_projections_onto_closed_subspaces_in_hilbert_spaces}, we will automatically have that $\pi(x) = x_E$. To this end, let us pair $x - \pi(x)$ with an arbitrary element of $E$ and then demonstrate that the pairing vanishes. Since $E \cong \bigoplus_{n = 1}^{\dim E} K e_n$, it suffices to check if $\<x - \pi(x), e_m\> = 0$ for any $1 \leq m \leq \dim E$. Thus, let us consider:
                    $$
                        \begin{aligned}
                            \<x - \pi(x), e_m\> & = \left\< x - \sum_{n = 1}^{\dim E} \frac{\<x, e_n\>}{\norm{e_n}^2} e_n, e_m \right\>
                            \\
                            & = \<x, e_m\> - \sum_{n = 1}^{\dim E} \frac{\<x, e_n\>}{\norm{e_n}^2} \<e_n, e_m\>
                            \\
                            & = \<x, e_m\> - \sum_{n = 1}^{\dim E} \frac{\<x, e_n\>}{\norm{e_n}^2} \delta_{n, m} \<e_m, e_m\>
                            \\
                            & = \<x, e_m\> - \frac{\<x, e_m\>}{\norm{e_m}^2} \<e_m, e_m\>
                            \\
                            & = \<x, e_m\> - \<x, e_m\>
                            \\
                            & = 0
                        \end{aligned}
                    $$
                which is what we need.

                To show that $\norm{x_E}^2 = \sum_{n = 1}^{\dim E} \frac{ \abs{ \<x, e_n\> }^2 }{\norm{e_n}^2}$, we exploit the linear indepedence of the sequence $\{e_n\}_{n = 1}^{\dim E}$ in order to identify:
                    $$E \cong K e_1 \oplus_2 ... \oplus_2 K e_{\dim E}$$
                (where $\oplus_2$ means we are endowing the direct sum with the $\ell^2$-norm; cf. convention \ref{conv: ell_p_direct_sums}). We then have:
                    $$
                        \begin{aligned}
                            \norm{x_E}^2 & = \norm{\sum_{n = 1}^{\dim E} \frac{\<x, e_n\>}{\norm{e_n}^2} e_n}^2
                            \\
                            & = \norm{ \left( \frac{\<x, e_n\>}{\norm{e_n}^2} e_n \right)_{n = 1}^{\dim E} }_{ K e_1 \oplus_2 ... \oplus_2 K e_{\dim E} }^2
                            \\
                            & = \sum_{n = 1}^{\dim E} \frac{ \abs{ \<x, e_n\> }^2 }{\norm{e_n}^4} \norm{e_n}^2
                            \\
                            & = \sum_{n = 1}^{\dim E} \frac{ \abs{ \<x, e_n\> }^2 }{\norm{e_n}^2}
                        \end{aligned}
                    $$
            \end{proof}
        Another corollary of proposition \ref{prop: orthogonal_projections_onto_closed_subspaces_in_hilbert_spaces} (and corollary \ref{coro: orthogonal_projections_onto_finite_dimensional_subspaces_in_hilbert_spaces}) is \textbf{Bessel's Inequality}.
        \begin{corollary}[Bessel's Inequality] \label{coro: bessel_inequality}
            With notations as in proposition \ref{prop: orthogonal_projections_onto_closed_subspaces_in_hilbert_spaces}, we have that:
                $$\sum_{n \geq 1} \frac{ \abs{ \<x, e_n\> }^2 }{\norm{e_n}^2} \leq \norm{x}^2$$
        \end{corollary}
            \begin{proof}
                From corollary \ref{coro: orthogonal_projections_onto_finite_dimensional_subspaces_in_hilbert_spaces}, we know that:
                    $$\norm{x_E}^2 = \sum_{n = 1}^{\dim E} \frac{ \abs{ \<x, e_n\> }^2 }{\norm{e_n}^2}$$
                so we have:
                    $$0 \leq \norm{x - x_E}^2 \leq \norm{x}^2 - \norm{x_E}^2 = \norm{x}^2 - \sum_{n = 1}^{\dim E} \frac{ \abs{ \<x, e_n\> }^2 }{\norm{e_n}^2}$$
                wherein the second inequality follows from Minkowski's Inequality. Applying $\lim_{\dim E \to +\infty}$ to both sides then yields:
                    $$\sum_{n \geq 1} \frac{ \abs{ \<x, e_n\> }^2 }{\norm{e_n}^2} \leq \norm{x}^2$$
            \end{proof}
            
        \begin{example}
            Let $\mu$ be the Lebesgue measure and consider the Hilbert space $L^2([-1, 1], \mu)$. Let $f, g, h \in L^2([-1, 1], \mu)$ be given by:
                $$f(t) := 1, g(t) := t, h(t) := t^2$$
            Let us compute:
                $$\dist( h, E )$$
            where $E := \bbC f \oplus \bbC g$.

            From proposition \ref{prop: orthogonal_projections_onto_closed_subspaces_in_hilbert_spaces}, we know that:
                $$
                    \begin{aligned}
                        \dist(h, E) & = \norm{h - h_E}
                        \\
                        & = \norm{ h - \frac{\<h, f\>}{\norm{f}^2} f + \frac{\<h, g\>}{\norm{g}^2} g }
                        \\
                        & = \norm{ h(t) - \frac{\int_{-1}^1 \abs{h(t) f(t)} dt}{\int_{-1}^1 \abs{f(t)^2} dt} f(t) + \frac{\int_{-1}^1 \abs{h(t) g(t)} dt}{\int_{-1}^1 \abs{g(t)^2} dt} g(t) }
                        \\
                        & = [...]
                    \end{aligned}
                $$
        \end{example}

        \begin{definition}[Fourier series] \label{def: fourier_series}
            Let $H$ be a Hilbert space and let $\{e_n\}_{n \geq 1} \not \ni 0$ be an orthogonal sequence. The \textbf{Fourier series} of an element $x \in H$ with respect to the given orthogonal sequence $\{e_n\}_{n \geq 1}$ is then:
                $$\scrF(x) := \sum_{n \geq 1} \frac{\<x, e_n\>}{\norm{e_n}^2} e_n$$
        \end{definition}
        Of course, \textit{a priori} there is no guarantee that Fourier series may or may not converge, nor may we expect that $\scrF(x) = x$ so that the Fourier series would be a representation of $x$ in terms of the linearly independent vectors $e_n$.
        \begin{theorem}[Convergence of Fourier series] \label{theorem: convergence_of_fourier_series}
            Let $H$ be a Hilbert space and let $\{e_n\}_{n \geq 1} \not \ni 0$ be an orthogonal sequence. For an element $x \in H$, the following statements are equivalent:
            \begin{enumerate}
                \item $\scrF(x)$ converges to $x$.
                \item $\norm{\scrF(x)}^2 = \norm{x}^2$.
                \item $x \in \overline{\bigoplus_{n \geq 1} K e_n}$.
            \end{enumerate}
        \end{theorem}
            \begin{proof}
                Clearly 1 implies 2 and 2 implies 3, so let us focus on showing that 3 implies 1. For each $N \in \N$, let $E_N := \bigoplus_{n = 1}^N K e_n$. These subspaces of $H$ are finite-dimensional, hence closed, so for any $x \in H$, we have:
                    $$x_{E_N} = \sum_{n = 1}^N \frac{\<x, e_n\>}{\norm{e_n}^2} e_n$$
                according to corollary \ref{coro: orthogonal_projections_onto_finite_dimensional_subspaces_in_hilbert_spaces}, from which we infer that:
                    $$\scrF(x) = \lim_{N \to +\infty} x_{E_N}$$
                Then, via continuity of norms, we see that:
                    $$\norm{\scrF(x)}^2 = \norm{\lim_{N \to +\infty} x_{E_N}}^2 = \lim_{N \to +\infty} \norm{x_{E_N}}^2 = \lim_{N \to +\infty} \sum_{n = 1}^N \frac{\abs{ \<x, e_n\> }^2}{\norm{e_n}^2} = \sum_{n \geq 1} \frac{\abs{ \<x, e_n\> }^2}{\norm{e_n}^2}$$
                Bessel's Inequality tells us that $\sum_{n \geq 1} \frac{\abs{ \<x, e_n\> }^2}{\norm{e_n}^2} \leq \norm{x} < +\infty$, so:
                    $$\norm{\scrF(x)} \leq \norm{x} < +\infty$$
                i.e. $\scrF(x)$ is convergent.

                To show that $\scrF(x)$ indeed converges to $x$, recall from proposition \ref{prop: orthogonal_projections_onto_closed_subspaces_in_hilbert_spaces} that $x = x_{E_N} + x_{E_N^{\perp}}$, and since $E_N \cap E_N^{\perp} = \{0\}$, so it suffices to show that $\lim_{N \to +\infty} x_{E_N^{\perp}} = 0$, which is equivalent to showing that $\lim_{N \to +\infty} \norm{x_{E_N^{\perp}}} = 0$. From proposition \ref{prop: orthogonal_projections_onto_closed_subspaces_in_hilbert_spaces}, we know that $\norm{x_{E_N^{\perp}}} = \dist(x, E_N)$, but since $x \in \overline{\bigoplus_{n \geq 1} K e_n} = \overline{\bigcup_{N \geq 1} E_N}$, we have that $\lim_{N \to +\infty} \dist(x, E_N) = 0$.
            \end{proof}
        \begin{theorem}[Uniqueness of Fourier series] \label{theorem: uniqueness_of_fourier_series}
            Let $H$ be a Hilbert space and let $\{e_n\}_{n \geq 1} \not \ni 0$ be an orthogonal sequence. For an element $x := \sum_{n \geq 1} a_n e_n \in \overline{\bigoplus_{n \geq 1} K e_n}$, we have:
                $$a_m = \frac{ \<x, e_m\> }{ \norm{e_m}^2 }$$
            for all $m \geq 1$. Consequently, the Fourier series $\scrF(x) := \sum_{n \geq 1} \frac{\<x, e_n\>}{\norm{e_n}^2} e_n$ with respect to $\{e_n\}_{n \geq 1}$ is the only way to write $x$ as a linear combination of the elements of $\{e_n\}_{n \geq 1}$.
        \end{theorem}
            \begin{proof}
                Fix $m \geq 1$ and then consider:
                    $$\<x, e_m\> = \sum_{n \geq 1} a_n \<e_n, e_m\> = \sum_{n \geq 1} a_n \delta_{n, m} \<e_m, e_m\> = a_m \<e_m, e_m\> = a_m \norm{e_m}^2$$
                Rearranging then yields:
                    $$a_m = \frac{\<x, e_m\>}{\norm{e_m}^2}$$
            \end{proof}

        A slightly more refined and more well-behaved notion that that of orthogonal sequences is that of \say{orthonormal bases}. 
        \begin{definition}[Orthonormal bases] \label{def: orthonormal_bases}
            An orthogonal subset $\Gamma$ of a Hilbert space $H$ is said to be \textbf{orthonormal} if and only if $\norm{x} = 1$ for all $x \in \Gamma$. An \textbf{orthonormal basis} is a maximal orthonormal subset (with respect to inclusion).
        \end{definition}
        \begin{remark}
            Orthogonal subsets are linearly independent \textit{a priori} by lemma \ref{lemma: orthogonal_subsets_linear_independence}, hence so are orthonormal subsets and even more particularly, orthonormal bases.
        \end{remark}
        In conjunction with linear independence, the following lemma justifies the use of the term \say{basis}.
        \begin{lemma}[Orthonormal bases are topological spanning subsets] \label{lemma: orthonormal_bases_are_topological_spanning_subsets}
            Let $H$ be a Hilbert space and let $\Gamma \subset H$ be an orthonormal basis therein. Then:
                $$H = \overline{ \span \Gamma }$$
            or in other words, orthonormal bases are topological/Schauder bases.
        \end{lemma}
            \begin{proof}
                Suppose for the sake of deriving a contradiction that $H \not = L := \overline{ \span \Gamma }$. This means that there exists some $y \in H \setminus L$. Since $L \subset H$ is a closed subspace, we have $H \cong L \oplus L^{\perp}$ as normed spaces, by proposition \ref{prop: orthogonal_projections_onto_closed_subspaces_in_hilbert_spaces}, and hence $y \in L^{\perp}$. This means that $\Gamma \cup \{y\}$ is an orthonormal subset of $H$ properly containing $\Gamma$ as a subset. However, $\Gamma$ is supposed to be maximal by virtue of being an orthonormal basis, so we have a contradiction.
            \end{proof}
        \begin{proposition}[Existence of orthonormal bases] \label{prop: existence-of_orthonormal_bases}
            Every Hilbert space admits an orthonormal basis.
        \end{proposition}
            \begin{proof}
                Consider the set of orthonormal subsets of $H$, partially ordered by inclusion. Since arbitrary unions of orthonormal subsets remain orthonormal, any ascending chain $\{\Gamma_i\}_{i \in I}$ therein has an upper bound, namely $\bigcup_{i \in I} \Gamma_i$. By Zorn's Lemma, there is a maximal element of $\calB$, which is the sought-for orthonormal basis for $H$.
            \end{proof}
        \begin{remark}
            Equivalently, one can define an orthonormal basis of a Hilbert space to be an orthonormal Schauder basis. The proof of existence is similar to the proof of proposition \ref{prop: existence-of_orthonormal_bases}.

            Also, if $\Gamma \subset H \setminus \{0\}$ is an orthogonal subset of a Hilbert space, then $\bar{\Gamma} := \left\{\frac{x}{\norm{x}} \mid x \in \Gamma\right\}$ will be an orthonormal basis of $H$ if and only if $\Gamma$ is a topological spanning subset of $H$, i.e. if and only if $H = \overline{\span \Gamma}$. 
        \end{remark}
        \begin{proposition}[Separable Hilbert spaces are countably dimensional] \label{prop: separable_hilbert_spaces_are_countably_dimensional}
            Any orthonormal basis of a separable Hilbert space is necessarily countable.
        \end{proposition}
            \begin{proof}
                
            \end{proof}

        \begin{lemma}[Norms of dual elements in Hilbert spaces] \label{lemma: norms_of_dual_elements_in_hilbert_spaces}
            Let $H$ be a Hilbert space. Then, for all $y \in H$, we have that:
                $$\norm{\<-, y\>_H}_{H^*_{\cont}} = \norm{y}_H$$
        \end{lemma}
            \begin{proof}
                We use the fact that $\norm{\<-, y\>_H}_{H^*_{\cont}} = \sup_{x \in H, \norm{x}_H} \abs{\<x, y\>_H}$ along with the Cauchy-Schwarz Inequality, which tells us that $\abs{\<x, y\>_H} \leq \norm{x}_H \norm{y}_H = \norm{y}_H$ for all $x \in H$ such that $\norm{x}_H = 1$
            \end{proof}
        \begin{theorem}[Riesz's Representability Theorem: Hilbert spaces are reflexive] \label{theorem: riesz_representation_theorem}
            Let $H$ be a Hilbert space. Then, any continuous linear functional $\varphi \in H^*_{\cont}$ will be representable, in the sense that there exists a unique $y_{\varphi} \in H$ such that:
                $$\varphi = \<-, y_{\varphi}\>_H$$
            In other words, for all $y \in H$, the linear functional $\<-, y\>_H$ is continuous with respect to the topology generated by $\norm{-}_H$, and one has a linear isometry\footnote{\say{$D$} for \say{duality}.}:
                $$D_H: H \xrightarrow[]{\cong} H^*_{\cont}$$
            which is given by $y \mapsto \<-, y\>_H$.
        \end{theorem}
            \begin{proof}
                \todo[inline]{Prove the Riesz Representation Theorem} 
            \end{proof}
        \begin{corollary}[Uniformity of Hilbert spaces] \label{coro: hilbert_space_uniformity}
            In Hilbert spaces, sequences converge if and only if they converge weakly. Phrased dually, sequences of continuous functionals on Hilbert spaces converge uniformly if and only if they converge pointwise.
        \end{corollary}
            \begin{proof}
                Strong convergence automatically implies weak convergence, so let us focus on the converse direction.
            
                To that end, pick a weakly convergent sequence $\{\varphi_n\}_{n \geq 0} \xrightarrow[]{\weak} \varphi$ in $H^*$, which means that:
                    $$\forall x \in H: \forall \e > 0: n \gg 0 \implies \abs{\ev_x[\varphi_n - \varphi]} = \abs{\varphi_n(x) - \varphi(x)} < \e$$
                From this, we infer that:
                    $$\norm{ \varphi_n - \varphi }_{H^*_{\cont}} = \sup_{x \in H, \norm{x}_H = 1} \abs{\varphi_n(x) - \varphi(x)} < \e$$
                which shows that there is strong convergence:
                    $$\{\varphi_n\}_{n \geq 0} \to \varphi$$
                Thus, we have shown that weak convergence in $H^*$ implies convergence therein, and since $H^*_{\cont}$ is linearly isometric to $H$ by theorem \ref{theorem: riesz_representation_theorem}, we see thus that the same statement holds for $H$.
            \end{proof}

    \subsection{Compact linear maps and some Fredholm theory}
        \begin{proposition}[Compact linear maps] \label{prop: compact_linear_maps}
            \todo[inline]{Compact linear maps}
        \end{proposition}
            \begin{proof}
                
            \end{proof}

        \begin{proposition}[Weakly bounded sequences] \label{prop: weakly_bounded_sequences}
            \todo[inline]{Weakly bounded sequences}
        \end{proposition}
            \begin{proof}
                
            \end{proof}