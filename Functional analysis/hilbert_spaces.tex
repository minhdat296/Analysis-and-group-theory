\section{Hilbert spaces}
    \subsection{Compact linear maps and some Fredholm theory}
        \begin{proposition}[Compact linear maps] \label{prop: compact_linear_maps}
            \todo[inline]{Compact linear maps}
        \end{proposition}
            \begin{proof}
                
            \end{proof}

        \begin{proposition}[Weakly bounded sequences] \label{prop: weakly_bounded_sequences}
            \todo[inline]{Weakly bounded sequences}
        \end{proposition}
            \begin{proof}
                
            \end{proof}

    \subsection{Hilbert spaces}
        \begin{convention}
            The complex conjugation of some complex number $z := a + ib$ shall be denoted by $z^{\dagger} := a - ib$.
        \end{convention}
    
        \begin{definition}[Hilbert spaces] \label{def: hilbert_spaces}
            Let $\sigma$ be a field automorphism of $K$.
        
            A \textbf{Hilbert space} is a vector space space $H$ equipped with a $\sigma$-braided, positive-definite, and non-degenerate $K$-bilinear form $\<-, -\>_H: H \x H \to K$ (called an \textbf{inner product}), with the braidedness condition meaning that:
                $$\forall x, y \in H: \<x, y\>_H = \sigma( \<y, x\>_H )$$
            The inner product $\<-, -\>$ is said to be \textbf{symmetric} if and only if $\sigma = \id_K$, and \textbf{Hermitian} if and only if $\sigma = (-)^{\dagger}$; note that for complete archimedean local fields, these are the only two possible field automorphisms (since $\Gal(\bbC/\R) \cong \Z/2$). 
        \end{definition}
        \begin{proposition}[Induced norms] \label{prop: induced_norms_on_hilbert_spaces}
            Let $H$ be a Hilbert space. Then, there will be an induced norm $\norm{-}_H$ on $H$ given by:
                $$\forall x \in H: \norm{x}_H := \sqrt{\<x, x\>_H}$$
            When equipped with the topology induced by this norm, Hilbert spaces become normed spaces. 
        \end{proposition}
            \begin{proof}
                Firstly, let us note that $\norm{-}_H$ is a well-defined function $H \to \R_{\geq 0}$ due to the positive-semi-definiteness of $\<-, -\>_H$. Next, the triangle inequality is due to the Cauchy-Schwarz inequality. Finally, to show that $\norm{x}_H = 0$ if and only if $x = 0$, we can simply make use of positive-definiteness.
            \end{proof}
        \begin{convention}
            \textit{A priori}, Hilbert spaces are not complete (at least according to definition \ref{def: hilbert_spaces}; it is common in the literature to require that Hilbert spaces are complete right from the beginning, but we find this to be circular\footnote{From the beginning, Hilbert spaces do not carry any topology, so it does not make any sense to require them to be complete. Only after we have shown that inner products induce norms on Hilbert spaces can we meaningfully require them to be complete.}). However, let us assume from now on that every Hilbert space is complete with respect to its norm topology. In particular, this means that Hilbert spaces are now Banach spaces whose norms come from inner products.
        \end{convention}
        The following is arguably the most ubiquitous and most useful family of examples of Hilbert spaces.  
        \begin{example}[$L^2$-spaces] \label{example: L_2_spaces_as_hilbert_spaces}
            Let $(X, \mu)$ be a measure space. In theorem \ref{theorem: L_p_space_duality}, it is shown that for any $p, q \in \N_{\geq 1} \cup \{+\infty\}$ such that $\frac1p + \frac1q = 1$, one has a linear homeomorphism:
                $$L^q(X, \mu) \xrightarrow[]{\cong} L^p(X, \mu)^*_{\weak}$$
            that is given by:
                $$g \mapsto \int_X (-) g d\mu$$

            Now, observe that when $p = q = 2$, not only does this weak duality holds, but it is in fact a weak \textit{self}-duality of $L^2(X, \mu)$, which allows us to define a bilinear pairing on this Banach space by:
                $$\<f, g\>_{L^2(X, \mu)} := \int_X \abs{ fg } d\mu$$
            using which the linear homeomorphism from before can be now alternatively given by $g \mapsto \<-, g\>$. One then verifies that:
                $$\norm{f}_{L^2(X, \mu)}^2 = \int_X \abs{f}^2 d\mu = \<f, f\>_{L^2(X, \mu)}$$
            to see that the induced norm coincides with the $L^2$-norm.
        \end{example}
        \begin{example}[Finite-dimensional Hilbert spaces]
            Any finite-dimensional inner product space is a Hilbert space. In fact, any finite-dimensional vector space can be upgraded to a Hilbert space in a canonical manner: by letting the inner product be the dot product (but of course, there are inner products that are not the dot product).
        \end{example}

        \begin{lemma}[Norms of dual elements in Hilbert spaces] \label{lemma: norms_of_dual_elements_in_hilbert_spaces}
            Let $H$ be a Hilbert space. Then, for all $y \in H$, we have that:
                $$\norm{\<-, y\>_H}_{H^*_{\cont}} = \norm{y}_H$$
        \end{lemma}
            \begin{proof}
                We use the fact that $\norm{\<-, y\>_H}_{H^*_{\cont}} = \sup_{x \in H, \norm{x}_H} \abs{\<x, y\>_H}$ along with the Cauchy-Schwarz Inequality, which tells us that $\abs{\<x, y\>_H} \leq \norm{x}_H \norm{y}_H = \norm{y}_H$ for all $x \in H$ such that $\norm{x}_H = 1$
            \end{proof}
        \begin{theorem}[Riesz's Representability Theorem] \label{theorme: riesz_representation_theorem}
            Let $H$ be a Hilbert space. Then, any continuous linear functional $\varphi \in H^*_{\cont}$ will be representable, in the sense that there exists a unique $y_{\varphi} \in H$ such that:
                $$\varphi = \<-, y_{\varphi}\>_H$$
            In other words, for all $y \in H$, the linear functional $\<-, y\>_H$ is continuous with respect to the topology generated by $\norm{-}_H$, and one has a linear isometry\footnote{\say{$D$} for \say{duality}.}:
                $$D_H: H \xrightarrow[]{\cong} H^*_{\cont}$$
            which is given by $y \mapsto \<-, y\>_H$.
        \end{theorem}
            \begin{proof}
                Let us exploit the fact that there is a linear isometry $\ev: H \xrightarrow[]{\cong} H^{**}_{\cont}$ given by $x \mapsto \ev_x$ with $\ev_x: H^*_{\cont} \to K$ being given by $\ev_x[\varphi] := \varphi(x)$ for all $\varphi \in H^*_{\cont}$ (see lemma \ref{lemma: continuous_duality_involutive}) in order to identify elements of $H$ with those of $H^{**}_{\cont}$ by means of pulling back along this linear isometry. It now suffices to construct a linear isometry:
                    $$P_H: H^{**}_{\cont} \to H^*_{\cont}$$
                We claim that the underlying linear isomorphism can be given by:
                    $$P_H(\ev_y) := \<-, y\>_H$$
                (we will need to verify the continuity of each of the functional $\<-, y\>_H$ too, to be able to conclude that the codomain of $P_H$ is actually $H^*_{\cont}$, not merely all of $H^*$); linearity is easy to see, so let us focus on proving that it is bijective.
                
                Firstly, to prove that $P_H$ is injective:
                    $$\ker P_H \cong \{ y \in H \mid \forall x \in H: \<x, y\>_H = 0 \}$$
                Since the bilinear form $\<-, -\>_H$ is non-degenerate, we can conclude immediately that:
                    $$\ker P_H \cong 0$$
                i.e. that $P_H$ is injective. This also guarantees that any vector $y_{\varphi} \in H$ such that:
                    $$\varphi = \<-, y_{\varphi}\>_H$$
                is necessarily unique: $\ev: H \xrightarrow[]{\cong} H^{**}_{\cont}$ is a linear isometry - so in particular, it is injective - meaning that the composition $P_H \circ \ev$ is also injective.
                
                To see that $P_H$ is surjective, let us note first of all that each $\<-, y\>_H: H \to K$ is continuous by virtue of being bounded (which is per the definition of inner products), and hence $\im P_H \subseteq H^*_{\cont}$. Next, observe that $P_H$ is continuous: to prove this, note firstly that since $H$ is complete by virtue of being a Hilbert space, any Cauchy sequence is convergent\footnote{Hilbert spaces are normed space, hence Hausdorff, so limit points will be unique if they exist.}, and then let $\{y_m\}_{M > 0} \to y$ is a (convergent) Cauchy sequence in $H$ and then consider\footnote{We are using the fact that $\{\ev_{y_m}\}_{M > 0} \to \ev_y$ if and only if $\{y_m\}_{M > 0} \to y$ as the map $\ev: H \to H^{**}_{\cont}$ is an isometry.}:
                    $$\forall \e > 0: m, n \gg 0 \implies \abs{P_H(\ev_{y_m} - \ev_{y_n})(x)} = \abs{ \< x, y_m - y_n \>_H } \leq \norm{y_m - y_n}_H \norm{ \<x, -\>_H }_{H^*_{\cont}} < \e \norm{x}_H$$
                for all $x \in H$, from which one sees that:
                    $$\forall \e > 0: m, n \gg 0 \implies \norm{ P_H(\ev_{y_m} - \ev_{y_n}) }_{H^*_{\cont}} < \e$$
                which proves that $P_H$ is continuous. We now claim that $\im P_H$ is dense inside $H^*_{\cont}$; this will help us prove surjectivity because $\{\ev_{y_m}\}_{M > 0} \to \ev_y$ if and only if $\{y_m\}_{M > 0} \to y$ for Cauchy sequences in $H^{**}_{\cont}$ and in $H$ respectively, as the map $\ev: H \to H^{**}_{\cont}$ is an isometry, and hence for any $\varphi \in H^*_{\cont}$, there exists a unique $y_{\varphi} \in H$ such that $\varphi = \<-, y_{\varphi}\>$. To do this, we will be using lemma \ref{lemma: density_orthogonal_complement_criterion}, which tells us that it shall suffice to show that:
                    $$(\im P_H)^{\perp}_{\cont} \cong 0$$
                Let us suppose for the sake of deriving a contradiction that $(\im P_H)^{\perp}_{\cont} \not \cong 0$, i.e.:
                    $$\exists \varphi \in H^*_{\cont} \setminus \{0\}: \varphi(\im P_H) = 0$$
                The construction of $P_H$, however, stipulates that there is a linear isometry:
                    $$H \cong \im P_H$$
                which suggests to us that:
                    $$\varphi(\im P_H) = 0 \iff (\forall x \in H: \varphi(x) = 0) \iff \varphi = 0$$
                But this contradicts the assumption that $\varphi \not = 0$, so it must be the case that:
                    $$(\im P_H)^{\perp}_{\cont} \cong 0$$
                and hence $\im P_H$ is dense inside $H^*_{\cont}$.

                Finally, because we have that:
                    $$\forall y \in H: \norm{ \<-, y\>_H }_{H^*_{\cont}} = \norm{y}_H = \norm{\ev_y}_{H^{**}_{\cont}}$$
                (the last equality is due to $\ev$ being an isometry) there is indeed a linear isometry:
                    $$D_H: H \to H^*_{\cont}$$
                which we now know to be given by $D_H := P_H \circ \ev$. 
            \end{proof}
        \begin{corollary}[Uniformity of Hilbert spaces] \label{coro: hilbert_space_uniformity}
            In Hilbert spaces, sequences converge if and only if they converge weakly. Phrased dually, sequences of continuous functionals on Hilbert spaces converge uniformly if and only if they converge pointwise.
        \end{corollary}
            \begin{proof}
                Strong convergence automatically implies weak convergence, so let us focus on the converse direction.
            
                To that end, pick a weakly convergent sequence $\{\varphi_n\}_{n \geq 0} \xrightarrow[]{\weak} \varphi$ in $H^*$, which means that:
                    $$\forall x \in H: \forall \e > 0: n \gg 0 \implies \abs{\ev_x[\varphi_n - \varphi]} = \abs{\varphi_n(x) - \varphi(x)} < \e$$
                From this, we infer that:
                    $$\norm{ \varphi_n - \varphi }_{H^*_{\cont}} = \sup_{x \in H, \norm{x}_H = 1} \abs{\varphi_n(x) - \varphi(x)} < \e$$
                which shows that there is strong convergence:
                    $$\{\varphi_n\}_{n \geq 0} \to \varphi$$
                Thus, we have shown that weak convergence in $H^*$ implies convergence therein, and since $H^*_{\cont}$ is linearly isometric to $H$ by theorem \ref{theorme: riesz_representation_theorem}, we see thus that the same statement holds for $H$.
            \end{proof}

        Our goal from now until the end of the section shall be to justify the attitude that many \say{reasonable} Hilbert spaces should be regarded as $L^2$-spaces over some particular measure spaces, with the point being that once a Hilbert space $H$ is realisable as an $L^2$-space, say $H \cong L^2(X, \mu)$ isometrically, the inner product on $H$ can be computed as the $L^1$-pairing on $(X, \mu)$, i.e.:
            $$\<f, g\>_H = \int_X \abs{fg} d\mu$$
        In many cases, the integral is usually easier to compute. In a more casual manner, such realisations allows us to regard general Hilbert spaces as direct generalisations of Euclidean spaces, and often, one can use Euclidean intuition for the purposes of studying Hilbert spaces. We caution the reader, though, that not every Hilbert space is an $L^2$-spaces. So-called \say{Sobolev spaces} (see subsection \ref{subsection: sobolev_spaces}), for instances, are such examples. 
        \begin{convention}
            From now on, we will mostly be working inside a fixed Hilbert space and not be concerned too much with morphisms between different Hilbert spaces. Therefore, in order to avoid visual clutter, let us suppress the subscripts indicating where norms and inner products are being taken until the end of the section, unless we fear it may be confusing to do so.
        \end{convention}
        We begin with the following fundamental definition.
        \begin{definition}[Orthogonal subsets of Hilbert spaces] \label{def: orthogonal_subsets}
            A subset $A$ of a Hilbert space $H$ is said to be \textbf{orthogonal} if and only if for all $x \not = y \in A$, we have $\<x, y\> = 0$.
        \end{definition}
        \begin{lemma}[Linear independence of orthogonal subsets] \label{lemma: orthogonal_subsets_linear_independence}
            Any orthogonal subset of a Hilbert space is necessarily linearly independent. 
        \end{lemma}
            \begin{proof}
                Let $H$ be a Hilbert space and let $A \subset H$ be an orthogonal subset. Suppose for the sake of deriving a contradiction that $A$ is linearly dependent, i.e. there exists $x \in A$ such that $x = \sum_i a_i e_i$ for some collection $\{e_i\}_{i \in I} \subset A$ and $\{a_i\}_{i \in I} \subset K \setminus \{0\}$. Then, for any $j \not = i \in I$, we shall have:
                    $$\<x, e_j\> = \sum_{i \in I} a_i \<e_i, e_j\> = \sum_{i \in I} a_i \delta_{i, j} = a_j \not = 0$$
                If $\<x, e_j\> \not = 0$ then $x \in K e_j$, but this would imply that $a_i = 0$ for all $i \not = j$, a contradiction.
            \end{proof}
        Orthogonal sequences are rather special, because they form the basis\footnote{Pun intended} for what is known as \say{harmonic analysis} (or \say{Fourier theory}). At the most holistic level, this is essentially just the idea that one can represent elements of a Hilbert space in terms of its projections onto the coordinate axes, though in the infinite-dimensional setting, there are certain technical subtleties that need to be handled with care. 
        \begin{lemma} \label{lemma: convergent_orthogonal_series_in_hilbert_spaces}
            Let $H$ be a Hilbert space and let $\{x_k\}_{k \geq 1} \subset H$ be an orthogonal sequence. Then $\sum_{k = 1}^{+\infty} x_k$ is convergent if and only if $\sum_{k = 1}^{+\infty} \norm{x_k}^2$ converges, and when $\sum_{k = 1}^{+\infty} x_k$ converges, we have that $\norm{ \sum_{k = 1}^{+\infty} x_k }^2 = \sum_{k = 1}^{+\infty} \norm{x_k}^2$.
        \end{lemma}
            \begin{proof}
                If $\sum_{k = 1}^{+\infty} \norm{x_k}^2$ is convergent then the sequence of partial sums $\left\{ \sum_{k = 1}^n \norm{x_k}^2 \right\}_{n \geq 1} \subset \R$ will converge, and since $\R$ is complete, this sequence of partial sums converges if and only if it is Cauchy, i.e. for all $\e > 0$ there exists $N \in \N$ such that if $n \geq m \geq N$ then:
                    $$\abs{ \sum_{k = m}^n \norm{x_k}^2 } = \sum_{k = m}^n \norm{x_k}^2 < \e^2$$
                Next, recall that orthogonal sequences are linearly independent, so we have:
                    $$\sum_{k = m}^n \norm{x_k}^2 = \norm{ ( x_m, ..., x_n ) }_{\bbK x_m \oplus_2 ... \oplus_2 \bbK x_n}^2 = \norm{\sum_{k = m}^n x_k}^2$$
                with the latter equality being due to the fact that $x_m, ..., x_n$ all belong to the same normed space, namely $H$, and so $( x_m, ..., x_n ) = \sum_{k = m}^n x_k$ (as elements of $\bbK x_m \oplus_2 ... \oplus_2 \bbK x_n$) by linear independence. Therefore $\norm{\sum_{k = m}^n x_k}^2 < \e^2$. The function $(-)^{\frac12}$ is monotonic on $\R_{\geq 0}$, so by applying it to both sides of the inequality above, we shall get:
                    $$\norm{\sum_{k = m}^n x_k} < \e$$
                whenever $n \geq m \geq N$, and hence the sequence $\left\{ \sum_{k = 1}^n x_k \right\}_{n \geq 1}$ is Cauchy. This is a Cauchy sequence in a Hilbert space, which is complete by definition, and hence it is convergent, i.e. $\sum_{k = 1}^{+\infty} x_k$ converges in $H$.

                Conversely, if $\sum_{k = 1}^{+\infty} x_k$ is convergent, then because Hilbert spaces are complete, the sequence of partial sums $\left\{ \sum_{k = 1}^n x_k \right\}_{n \geq 1}$ shall be Cauchy, i.e. for all $\e > 0$ there exists $N \in \N$ such that $\norm{ \sum_{k = m}^n x_k } < \e^{\frac12}$ for all $n \geq m \geq N$. Using the fact that orthogonal sequences are linearly independent and thus $\sum_{k = m}^n \norm{x_k}^2 = \norm{ ( x_m, ..., x_n ) }_{\bbK x_m \oplus_2 ... \oplus_2 \bbK x_n}^2 = \norm{\sum_{k = m}^n x_k}^2$ again, we see that for the same $N \in \N$, we have that $\sum_{k = m}^n \norm{x_k}^2 < (\e^{\frac12})^2 = \e$ for all $n \geq m \geq N$. The sequence of partial sums $\left\{ \sum_{k = 1}^n \norm{x_k}^2 \right\}_{n \geq 1} \subset \R$ is thus Cauchy, and hence convergent, as $\R$ is complete.

                Finally, we have that $\sum_{k = 1}^n \norm{x_k}^2 = \norm{ ( x_1, ..., x_n ) }_{\bbK x_1 \oplus_2 ... \oplus_2 \bbK x_n}^2 = \norm{\sum_{k = 1}^n x_k}^2$ for all $n \geq 1$, so when $\sum_{k = 1}^{+\infty} x_k := \lim_{n \to +\infty} \sum_{k = 1}^n x_k$ is convergent, we can exploit the continuity of norms to get:
                    $$\norm{\lim_{n \to +\infty} \sum_{k = 1}^n x_k} = \lim_{n \to +\infty} \norm{\sum_{k = 1}^n x_k} = \lim_{n \to +\infty} \sum_{k = 1}^n \norm{x_k}^2 = \sum_{k = 1}^{+\infty} \norm{x_k}^2$$
                as claimed.
            \end{proof}

        The following result is more-or-less purely technical, but it is very useful for handling projections.
        \begin{proposition}[Orthogonal projections onto closed subspaces in Hilbert spaces] \label{prop: orthogonal_projections_onto_closed_subspaces_in_hilbert_spaces}
            Let $H$ be a Hilbert space and let $L \subset H$ be a closed subspace. Then, the orthogonal complement/annihilator $L^{\perp}$ as in corollary \ref{coro: annihilators_vs_orthogonal_complements} is identified as:
                $$L^{\perp} \cong \{ x \in H \mid \forall y \in L: \<x, y\> = 0 \}$$
            and of course, we have $H \cong L \oplus L^{\perp}$ as normed spaces.

            Moreover, for all elements $x \in H$, we have that:
                $$\dist(x, L) := \inf_{y \in L} \norm{x - y} = \norm{x - \pi_L(x)} = \norm{\pi_{L^{\perp}}(x)}$$
            where $\pi_L: H \to L, \pi_{L^{\perp}}: H \to L^{\perp}$ are the canonical projections.
        \end{proposition}
            \begin{proof}
                \todo[inline]{Not written}
            \end{proof}
        \begin{convention}
            Typically, if $H$ is a Hilbert space and $L$ is a closed subspace therein, and if $\pi_L: H \to L, \pi_{L^{\perp}}: H \to L^{\perp}$ are as in proposition \ref{prop: orthogonal_projections_onto_closed_subspaces_in_hilbert_spaces}, then for all $x \in H$, we shall write:
                $$x_L := \pi_L(x), x_{L^{\perp}} := \pi_{L^{\perp}}(x)$$
        \end{convention}
        Finite-dimensional subspaces are closed, so we have the following corollary to proposition \ref{prop: orthogonal_projections_onto_closed_subspaces_in_hilbert_spaces}.
        \begin{corollary}[Projecting onto finite-dimensional subspaces] \label{coro: orthogonal_projections_onto_finite_dimensional_subspaces_in_hilbert_spaces}
            Let $H$ be a Hilbert space and let $E \subset $ be a finite-dimensional subspace, in which we choose an orthogonal sequence $\{e_n\}_{n = 1}^{\dim E} \not \ni 0$. Then:
                $$x_E = \sum_{n = 1}^{\dim E} \frac{\<x, e_n\>}{\norm{e_n}^2} e_n$$
            for all $x \in H$.

            Moreover, we have:
                $$\norm{x_E}^2 = \sum_{n = 1}^{\dim E} \frac{ \abs{ \<x, e_n\> }^2 }{\norm{e_n}^2}$$
        \end{corollary}
            \begin{proof}
                Let $\pi: H \to E$ be the linear map given by $\pi(x) := \sum_{n = 1}^{\dim E} \frac{\<x, e_n\>}{\norm{e_n}^2} e_n$. If we can show that $x - \pi(x) \in E^{\perp}$ then by proposition \ref{prop: orthogonal_projections_onto_closed_subspaces_in_hilbert_spaces}, we will automatically have that $\pi(x) = x_E$. To this end, let us pair $x - \pi(x)$ with an arbitrary element of $E$ and then demonstrate that the pairing vanishes. Since $E \cong \bigoplus_{n = 1}^{\dim E} K e_n$, it suffices to check if $\<x - \pi(x), e_m\> = 0$ for any $1 \leq m \leq \dim E$. Thus, let us consider:
                    $$
                        \begin{aligned}
                            \<x - \pi(x), e_m\> & = \left\< x - \sum_{n = 1}^{\dim E} \frac{\<x, e_n\>}{\norm{e_n}^2} e_n, e_m \right\>
                            \\
                            & = \<x, e_m\> - \sum_{n = 1}^{\dim E} \frac{\<x, e_n\>}{\norm{e_n}^2} \<e_n, e_m\>
                            \\
                            & = \<x, e_m\> - \sum_{n = 1}^{\dim E} \frac{\<x, e_n\>}{\norm{e_n}^2} \delta_{n, m} \<e_m, e_m\>
                            \\
                            & = \<x, e_m\> - \frac{\<x, e_m\>}{\norm{e_m}^2} \<e_m, e_m\>
                            \\
                            & = \<x, e_m\> - \<x, e_m\>
                            \\
                            & = 0
                        \end{aligned}
                    $$
                which is what we need.

                To show that $\norm{x_E}^2 = \sum_{n = 1}^{\dim E} \frac{ \abs{ \<x, e_n\> }^2 }{\norm{e_n}^2}$, we exploit the linear indepedence of the sequence $\{e_n\}_{n = 1}^{\dim E}$ in order to identify:
                    $$E \cong K e_1 \oplus_2 ... \oplus_2 K e_{\dim E}$$
                (where $\oplus_2$ means we are endowing the direct sum with the $\ell^2$-norm; cf. convention \ref{conv: ell_p_direct_sums}). We then have:
                    $$
                        \begin{aligned}
                            \norm{x_E}^2 & = \norm{\sum_{n = 1}^{\dim E} \frac{\<x, e_n\>}{\norm{e_n}^2} e_n}^2
                            \\
                            & = \norm{ \left( \frac{\<x, e_n\>}{\norm{e_n}^2} e_n \right)_{n = 1}^{\dim E} }_{ K e_1 \oplus_2 ... \oplus_2 K e_{\dim E} }^2
                            \\
                            & = \sum_{n = 1}^{\dim E} \frac{ \abs{ \<x, e_n\> }^2 }{\norm{e_n}^4} \norm{e_n}^2
                            \\
                            & = \sum_{n = 1}^{\dim E} \frac{ \abs{ \<x, e_n\> }^2 }{\norm{e_n}^2}
                        \end{aligned}
                    $$
            \end{proof}
        Another corollary of proposition \ref{prop: orthogonal_projections_onto_closed_subspaces_in_hilbert_spaces} (and corollary \ref{coro: orthogonal_projections_onto_finite_dimensional_subspaces_in_hilbert_spaces}) is \textbf{Bessel's Inequality}.
        \begin{corollary}[Bessel's Inequality] \label{coro: bessel_inequality}
            With notations as in proposition \ref{prop: orthogonal_projections_onto_closed_subspaces_in_hilbert_spaces}, we have that:
                $$\sum_{n \geq 1} \frac{ \abs{ \<x, e_n\> }^2 }{\norm{e_n}^2} \leq \norm{x}^2$$
        \end{corollary}
            \begin{proof}
                From corollary \ref{coro: orthogonal_projections_onto_finite_dimensional_subspaces_in_hilbert_spaces}, we know that:
                    $$\norm{x_E}^2 = \sum_{n = 1}^{\dim E} \frac{ \abs{ \<x, e_n\> }^2 }{\norm{e_n}^2}$$
                so we have:
                    $$0 \leq \norm{x - x_E}^2 \leq \norm{x}^2 - \norm{x_E}^2 = \norm{x}^2 - \sum_{n = 1}^{\dim E} \frac{ \abs{ \<x, e_n\> }^2 }{\norm{e_n}^2}$$
                wherein the second inequality follows from Minkowski's Inequality. Applying $\lim_{\dim E \to +\infty}$ to both sides then yields:
                    $$\sum_{n \geq 1} \frac{ \abs{ \<x, e_n\> }^2 }{\norm{e_n}^2} \leq \norm{x}^2$$
            \end{proof}
        \begin{example}
            Let $\mu$ be the Lebesgue measure and consider the Hilbert space $L^2([-1, 1], \mu)$. Let $f, g, h \in L^2([-1, 1], \mu)$ be given by:
                $$f(t) := 1, g(t) := t, h(t) := t^2$$
            Let us compute:
                $$\dist( h, E )$$
            where $E := \bbC f \oplus \bbC g$.

            From proposition \ref{prop: orthogonal_projections_onto_closed_subspaces_in_hilbert_spaces}, we know that:
                $$
                    \begin{aligned}
                        \dist(h, E) & = \norm{h - h_E}
                        \\
                        & = \norm{ h - \frac{\<h, f\>}{\norm{f}^2} f + \frac{\<h, g\>}{\norm{g}^2} g }
                        \\
                        & = \norm{ h(t) - \frac{\int_{-1}^1 \abs{h(t) f(t)} dt}{\int_{-1}^1 \abs{f(t)^2} dt} f(t) + \frac{\int_{-1}^1 \abs{h(t) g(t)} dt}{\int_{-1}^1 \abs{g(t)^2} dt} g(t) }
                        \\
                        & = [...]
                    \end{aligned}
                $$
        \end{example}

        \begin{definition}[Fourier series] \label{def: fourier_series}
            Let $H$ be a Hilbert space and let $\{e_n\}_{n \geq 1} \not \ni 0$ be an orthogonal sequence. The \textbf{Fourier series} of an element $x \in H$ with respect to the given orthogonal sequence $\{e_n\}_{n \geq 1}$ is then:
                $$\scrF(x) := \sum_{n \geq 1} \frac{\<x, e_n\>}{\norm{e_n}^2} e_n$$
        \end{definition}
        Of course, \textit{a priori} there is no guarantee that Fourier series may or may not converge, nor may we expect that $\scrF(x) = x$ so that the Fourier series would be a representation of $x$ in terms of the linearly independent vectors $e_n$.
        \begin{theorem}[Convergence of Fourier series] \label{theorem: convergence_of_fourier_series}
            Let $H$ be a Hilbert space and let $\{e_n\}_{n \geq 1} \not \ni 0$ be an orthogonal sequence. For an element $x \in H$, the following statements are equivalent:
            \begin{enumerate}
                \item $\scrF(x)$ converges to $x$.
                \item $\norm{\scrF(x)}^2 = \norm{x}^2$.
                \item $x \in \overline{\bigoplus_{n \geq 1} K e_n}$.
            \end{enumerate}
        \end{theorem}
            \begin{proof}
                Clearly 1 implies 2 and 2 implies 3, so let us focus on showing that 3 implies 1. For each $N \in \N$, let $E_N := \bigoplus_{n = 1}^N K e_n$. These subspaces of $H$ are finite-dimensional, hence closed, so for any $x \in H$, we have:
                    $$x_{E_N} = \sum_{n = 1}^N \frac{\<x, e_n\>}{\norm{e_n}^2} e_n$$
                according to corollary \ref{coro: orthogonal_projections_onto_finite_dimensional_subspaces_in_hilbert_spaces}, from which we infer that:
                    $$\scrF(x) = \lim_{N \to +\infty} x_{E_N}$$
                Then, via continuity of norms, we see that:
                    $$\norm{\scrF(x)}^2 = \norm{\lim_{N \to +\infty} x_{E_N}}^2 = \lim_{N \to +\infty} \norm{x_{E_N}}^2 = \lim_{N \to +\infty} \sum_{n = 1}^N \frac{\abs{ \<x, e_n\> }^2}{\norm{e_n}^2} = \sum_{n \geq 1} \frac{\abs{ \<x, e_n\> }^2}{\norm{e_n}^2}$$
                Bessel's Inequality tells us that $\sum_{n \geq 1} \frac{\abs{ \<x, e_n\> }^2}{\norm{e_n}^2} \leq \norm{x} < +\infty$, so:
                    $$\norm{\scrF(x)} \leq \norm{x} < +\infty$$
                i.e. $\scrF(x)$ is convergent.

                To show that $\scrF(x)$ indeed converges to $x$, recall from proposition \ref{prop: orthogonal_projections_onto_closed_subspaces_in_hilbert_spaces} that $x = x_{E_N} + x_{E_N^{\perp}}$, and since $E_N \cap E_N^{\perp} = \{0\}$, so it suffices to show that $\lim_{N \to +\infty} x_{E_N^{\perp}} = 0$, which is equivalent to showing that $\lim_{N \to +\infty} \norm{x_{E_N^{\perp}}} = 0$. From proposition \ref{prop: orthogonal_projections_onto_closed_subspaces_in_hilbert_spaces}, we know that $\norm{x_{E_N^{\perp}}} = \dist(x, E_N)$, but since $x \in \overline{\bigoplus_{n \geq 1} K e_n} = \overline{\bigcup_{N \geq 1} E_N}$, we have that $\lim_{N \to +\infty} \dist(x, E_N) = 0$.
            \end{proof}
        \begin{theorem}[Uniqueness of Fourier series] \label{theorem: uniqueness_of_fourier_series}
            Let $H$ be a Hilbert space and let $\{e_n\}_{n \geq 1} \not \ni 0$ be an orthogonal sequence. For an element $x := \sum_{n \geq 1} a_n e_n \in \overline{\bigoplus_{n \geq 1} K e_n}$, we have:
                $$a_m = \frac{ \<x, e_m\> }{ \norm{e_m}^2 }$$
            for all $m \geq 1$. Consequently, the Fourier series $\scrF(x) := \sum_{n \geq 1} \frac{\<x, e_n\>}{\norm{e_n}^2} e_n$ with respect to $\{e_n\}_{n \geq 1}$ is the only way to write $x$ as a linear combination of the elements of $\{e_n\}_{n \geq 1}$.
        \end{theorem}
            \begin{proof}
                
            \end{proof}
        
        \begin{definition}[Gram-Schmidt orthonormalisation]
            
        \end{definition}
        \begin{corollary}[Orthonormal bases] \label{coro: orthonormal_bases_for_hilbert_spaces}
            Every Hilbert space admits a topological orthonormal basis.
        \end{corollary}
            \begin{proof}
                We claim that any topological basis $\{x_i\}_{i \in I}$ for a given Hilbert space $H$ can be refined into an orthonormal one. By theorem \ref{theorme: riesz_representation_theorem}, such a basis induces a dual topological basis $\{x_i^* := \<-, x_i\>_H\}_{i \in I}$ for $H^*_{\cont}$. One can then find a family of vectors $\{e_i\}_{i \in I} \subset H$ such that:
                    $$\forall i \in I: \<e_i, x_j\>_H := \delta_{i, j}$$
                The vectors $e_i$ can be obtained by the Gram-Schmidt procedure, which also guarantees that they are linearly independent from one another. Finally, to show that $\overline{\bigoplus_{i \in I} \bbC e_i} \cong H$ (i.e. that $\{e_i\}_{i \in I}$ indeed \textit{topological} spans $H$), let us note that by the Gram-Schmidt procedure, this is the case if and only if $\{\<-, x_i\>\}_{i \in I}$ is a topological basis for $H^*_{\cont}$. But by corollary \ref{coro: hilbert_space_uniformity}, this is the case if and only if $\{x_i\}_{i \in I}$ is a topological basis for $H$, which is true by assumption.
            \end{proof}

        Let us end the subsection with some examples. 
        \begin{example}[Riesz-Markov-Kakutani representation theorem: measures are functionals]
            
        \end{example}
        \begin{example}[$L^2$-spaces over compact Lie groups and Fourier series; Plancherel's Theorem]
            
        \end{example}